import numpy as np
import torch

def calculate_novelty_exploration_efficiency(state, visited_states):
    ''' Novelty-Based Exploration: Compute the novelty of the states 
        visited by the agent. The novelty can be defined as the distance 
        between the current state and the nearest state in the agent's 
        memory. The higher the novelty, the more diverse the states 
        visited by the agent, and hence the better the exploration.
        
        Args:
            state (np.ndarray): The current state.
            visited_states (set): The set of visited states.

        Returns:
            float: The exploration efficiency value.
        '''
    state_dist = np.inf
    for visited_state in visited_states:
        dist = np.linalg.norm(state - np.array(visited_state))
        state_dist = min(state_dist, dist)
    exploration_efficiency = 1 / state_dist if state_dist > 0 else np.inf
    return exploration_efficiency

def calculate_entropy_exploration_efficiency(logits):
    ''' Entropy-Based Exploration: Compute the entropy of the action distribution 
        generated by the policy network. The higher the entropy, the more diverse the actions
        taken by the agent, and hence the better the exploration. 

        The entropy can be calculated using the following formula:

        entropy = - sum(p*log(p))

        where p is the probability distribution over the actions, and log is the natural logarithm.
        
        Args:
            state (np.ndarray): The current state.
            visited_states (set): The set of visited states.

        Returns:
            float: The exploration efficiency value.
        
        '''
    probs = torch.softmax(logits, dim=-1)
    log_probs = torch.log(probs)
    entropy = -torch.sum(probs * log_probs, dim=-1)
    exploration_efficiency = torch.mean(entropy)
    return exploration_efficiency.item()

def calculate_coverage_exploration_efficiency(visited_states, state_space_size):
    ''' Coverage-Based Exploration: Divide the number of unique states visited 
        by the agent by the total number of possible states in the environment.
        
        Args:
            visited_states (set): The set of visited states.
            state_space_size (int): The total number of possible states in the environment.

        Returns:
            float: The exploration efficiency value.
    '''
    coverage = len(visited_states) / state_space_size
    exploration_efficiency = coverage
    return exploration_efficiency
