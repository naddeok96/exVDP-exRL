on a single L2 GPU for a batch size of 2:

Mu_out FC: 0.0046541690826416016 -> 0.0005080699920654297           (9.160488033786955x)
Sigma_out FC Layer: 0.4106266498565674 -> 0.0004341602325439453     (945.7951674903899x)
KL: 1.0239548683166504 -> 0.05106687545776367                       (20.05125355992343x)
Jac: 0.0065288543701171875 -> 0.004924297332763672                  (1.3258448726638907x)

Thru put: 71.35632991790771 -> 1.0897674560546875                   (65.47849224296056)


Train 5 Epochs -> 64884 -> 1744                                     (37.20412844036697x)